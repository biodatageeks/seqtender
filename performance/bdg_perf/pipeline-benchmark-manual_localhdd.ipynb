{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run: `seq 1 1 10 | xargs  -i papermill /data/work/home/rmalanij/bdg-seqtender/performance/bdg_perf/pipeline-benchmark-manual.ipynb /data/work/home/rmalanij/bdg-seqtender/performance/bdg_perf/pipeline-benchmark-manual_run.ipynb -p executor_num {} -k seq-edu\"'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "executor_num = 8\n",
    "executor_mem  = 4\n",
    "min_partitions = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "bdg_perf_pass = os.environ.get(\"BDG_PERF_PASS\")\n",
    "bdg_perf_db = os.environ.get(\"BDG_PERF_DB\")\n",
    "bdg_perf_user = os.environ.get(\"BDG_PERF_USER\")\n",
    "bdg_perf_table = \"bdg_perf_tests\"\n",
    "os.environ['SPARK_HOME'] = \"/data/local/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.6'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--conf spark.hadoop.yarn.timeline-service.enabled=false --conf spark.driver.extraJavaOptions='-Dhdp.version=3.1.0.0-78' --conf spark.yarn.am.extraJavaOptions='-Dhdp.version=3.1.0.0-78' --conf spark.hadoop.hive.metastore.uris='thrift://cdh01.cl.ii.pw.edu.pl:9083' --conf spark.hadoop.hive.metastore.sasl.enabled=true --conf spark.hadoop.metastore.catalog.default=hive --conf spark.sql.catalogImplementation=hive --conf spark.hadoop.hive.execution.engine=mr --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.maxExecutors=5 --conf spark.sql.hive.metastore.jars='/usr/hdp/current/spark2-client/standalone-metastore/*' --conf spark.hadoop.net.topology.script.file.name='/etc/hadoop/conf/topology_script_spark.py' --conf spark.jars.repositories='http://zsibio.ii.pw.edu.pl/nexus/repository/maven-releases/,http://zsibio.ii.pw.edu.pl/nexus/repository/maven-snapshots/' --conf spark.jars='/data/local/opt/sequila/bdg-sequila_2.11-0.6.0-spark-2.4.3-SNAPSHOT-assembly.jar,/data/local/opt/seqtender/bdg-seqtender_2.11-0.2-SNAPSHOT-assembly.jar' pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = 'SeqtenderPerfTest'\n",
    "\n",
    "split_size = round(134217728/min_partitions)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".master(f'local[{executor_num}]') \\\n",
    ".config('spark.driver.memory','4g') \\\n",
    ".config('spark.executor.memory', f'{executor_mem}g') \\\n",
    ".config('spark.dynamicAllocation.enabled', 'false') \\\n",
    ".config('spark.sparkContext.defaultMinPartitions',f'{min_partitions}') \\\n",
    ".config(\"spark.hadoop.mapreduce.input.fileinputformat.split.maxsize\", f'{split_size}') \\\n",
    ".config(\"spark.hadoop.mapreduce.input.fileinputformat.split.minsize\", f'{split_size}') \\\n",
    ".appName(f'bdgenomics-{app_name}') \\\n",
    ".getOrCreate()\n",
    "#.config('spark.executor.instances',f'{executor_num}') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vcf_path= \"/igap/all/split/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz\"\n",
    "#anno_vcf_path = '/edugen/vcf/NA12878_anno.vcf'\n",
    "\n",
    "vcf_path= \"/data/work/home/rmalanij/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz\"\n",
    "anno_vcf_path = '/data/work/home/rmalanij/NA12878_anno.vcf'\n",
    "\n",
    "\n",
    "cache_dir = \"/data/samples/vep_data/vep/95\"\n",
    "vep_version=\"95\"\n",
    "annotate_cmd = f\"\"\"docker run --rm -i -v {cache_dir}:/opt/vep/.vep biodatageeks/bdg-vep:{vep_version}\n",
    "        vep\n",
    "        --dir /opt/vep/.vep\n",
    "        --pick_allele\n",
    "        --format vcf\n",
    "        --no_stats\n",
    "        --force_overwrite\n",
    "        --everything\n",
    "        -cache\n",
    "        --vcf\n",
    "        -offline\n",
    "        -o stdout \"\"\".replace(\"\\n   \", \"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import hashlib\n",
    "import re\n",
    "import datetime \n",
    "\n",
    "\n",
    "def time(command, \n",
    "         tag: str = None, \n",
    "         executor_name: str = None,\n",
    "         tool_name: str = None,\n",
    "         tool_version: str = None,\n",
    "         docker_image: str = None,\n",
    "         num = 1, \n",
    "         executor_num = 1, \n",
    "         executor_mem = 1, \n",
    "         global_vars = None,\n",
    "         docker_command: str = None,\n",
    "         input_file: str = None):\n",
    "    results = []\n",
    "    for i in range(0, num):\n",
    "        wall_time = timeit.timeit(command, number=1, globals = global_vars )\n",
    "        command_hash = hashlib.md5(re.sub(r'\\W', '', command).encode() ).hexdigest()\n",
    "        perf_record = [command_hash, \n",
    "                       tag,\n",
    "                       executor_name,\n",
    "                       tool_name,\n",
    "                       tool_version,\n",
    "                       docker_image,\n",
    "                       datetime.datetime.now(), \n",
    "                       command, \n",
    "                       docker_command,\n",
    "                       input_file,\n",
    "                       executor_num, \n",
    "                       executor_mem, \n",
    "                       wall_time ]\n",
    "        results.append(perf_record)\n",
    "    dfw = spark.createDataFrame(results, ['test_id',\n",
    "                                          'tag',\n",
    "                                          'executor_name',\n",
    "                                          'tool_name',\n",
    "                                          'tool_version',\n",
    "                                          'docker_image',\n",
    "                                          'time_stamp', \n",
    "                                          'command',\n",
    "                                          'docker_command',\n",
    "                                          'input_file',\n",
    "                                          'exec_total_cores', \n",
    "                                          'exec_mem', \n",
    "                                          'wall_time'])\n",
    "#     dfw.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", f\"{bdg_perf_db}\") \\\n",
    "#     .option(\"dbtable\", f\"{bdg_perf_table}\") \\\n",
    "#     .option(\"user\", f\"{bdg_perf_user}\") \\\n",
    "#     .option(\"password\", f'{bdg_perf_pass}') \\\n",
    "#     .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "    return dfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!hdfs dfs -rm -r -skipTrash /edugen/vcf/NA12878_anno.vcf*\n",
    "!yes | rm -r {anno_vcf_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tag = 'vcf_annotation localhdd'\n",
    "anno_code = \"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from seqtender import SeqTenderAnnotation\n",
    "seq_anno = SeqTenderAnnotation(spark)\n",
    "annotated = seq_anno.pipe_variants (vcf_path, annotate_cmd)\n",
    "seq_anno.save_variants(anno_vcf_path, annotated)\n",
    "\"\"\"\n",
    "\n",
    "global_vars={'vcf_path': vcf_path,\n",
    "             'annotate_cmd': annotate_cmd,\n",
    "             'anno_vcf_path': anno_vcf_path\n",
    "            }\n",
    "\n",
    "df = time(anno_code,\n",
    "          num = 1,\n",
    "          executor_num = executor_num,\n",
    "          executor_name = 'seqtender',\n",
    "          tool_name = 'vep',\n",
    "          tool_version = vep_version,\n",
    "          docker_image = f'biodatageeks/bdg-vep:{vep_version}',\n",
    "          tag = tag,\n",
    "          executor_mem = executor_mem, \n",
    "          docker_command = annotate_cmd,\n",
    "          input_file = vcf_path,\n",
    "          global_vars = global_vars)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "seq-edu",
   "language": "python",
   "name": "seq-edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
